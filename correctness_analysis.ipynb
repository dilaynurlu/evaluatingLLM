{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2da7c610-0a2a-4cf6-b743-836e8e60f1be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0437af8c-440e-4211-976d-0d78ddeffa68",
   "metadata": {},
   "outputs": [],
   "source": [
    "cr_df_P0 = pd.read_csv(\"eval/results/correctness/P0/correctness_results_P0.csv\")\n",
    "\n",
    "cr_df_P1 = pd.read_csv(\"eval/results/correctness/P1/correctness_results_P1.csv\")\n",
    "\n",
    "cr_df_P2 = pd.read_csv(\"eval/results/correctness/P2/correctness_results_P2.csv\")\n",
    "\n",
    "cr_df_P3 = pd.read_csv(\"eval/results/correctness/P3/correctness_results_P3.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "718ee482-d757-46e3-9453-e0533ba6a959",
   "metadata": {},
   "outputs": [],
   "source": [
    "def correctness_rates(df: pd.DataFrame) -> dict:\n",
    "    \"\"\"\n",
    "    Returns rates (percentages) for a three-stage correctness pipeline:\n",
    "\n",
    "    1) Syntactic not ok:\n",
    "       syntactic_ok == 0, computed over all generated tests.\n",
    "\n",
    "    2) Execution not ok (given syntactic validity):\n",
    "       syntactic_ok == 1 AND execution_ok == 0,\n",
    "       computed over syntactically valid tests only.\n",
    "\n",
    "    3) Assertion not ok (given execution):\n",
    "       syntactic_ok == 1 AND execution_ok == 1 AND assertion_ok == 0,\n",
    "       computed over executed tests only.\n",
    "\n",
    "    Note:\n",
    "    assertion_ok encodes both assertion presence and passing behavior.\n",
    "    Therefore, assertion_not_ok includes tests with missing assertions\n",
    "    as well as tests with failing assertions.\n",
    "    \"\"\"\n",
    "    total = len(df)\n",
    "    if total == 0:\n",
    "        raise ValueError(\"DataFrame is empty\")\n",
    "\n",
    "    syntactic_not_ok = (df[\"syntactic_ok\"] == 0).sum()\n",
    "    syntactic_valid = (df[\"syntactic_ok\"] == 1).sum()\n",
    "\n",
    "    execution_not_ok = (\n",
    "        (df[\"syntactic_ok\"] == 1) &\n",
    "        (df[\"execution_ok\"] == 0)\n",
    "    ).sum()\n",
    "\n",
    "    # executed tests (syntactic_ok == 1 AND execution_ok == 1)\n",
    "    executed = df[\n",
    "        (df[\"syntactic_ok\"] == 1) &\n",
    "        (df[\"execution_ok\"] == 1)\n",
    "    ]\n",
    "    executed_total = len(executed)\n",
    "\n",
    "    assertion_not_ok_executed = (\n",
    "        (executed[\"assertion_ok\"] == 0).sum()\n",
    "        if executed_total > 0 else 0\n",
    "    )\n",
    "\n",
    "    fully_correct = (\n",
    "        (df[\"syntactic_ok\"] == 1) &\n",
    "        (df[\"execution_ok\"] == 1) &\n",
    "        (df[\"assertion_ok\"] == 1)\n",
    "    ).sum()\n",
    "\n",
    "    return {\n",
    "        \"total_tests\": total,\n",
    "\n",
    "        \"syntactic_not_ok_count\": syntactic_not_ok,\n",
    "        \"syntactic_not_ok_pct\": 100 * syntactic_not_ok / total,\n",
    "\n",
    "        \"syntactic_valid_tests\": syntactic_valid,\n",
    "        \"execution_not_ok_count\": execution_not_ok,\n",
    "        \"execution_not_ok_pct\": (\n",
    "            100 * execution_not_ok / syntactic_valid\n",
    "            if syntactic_valid > 0 else 0.0\n",
    "        ),\n",
    "\n",
    "        \"executed_tests\": executed_total,\n",
    "        \"assertion_not_ok_executed_count\": assertion_not_ok_executed,\n",
    "        \"assertion_not_ok_executed_pct\": (\n",
    "            100 * assertion_not_ok_executed / executed_total\n",
    "            if executed_total > 0 else 0.0\n",
    "        ),\n",
    "\n",
    "        \"fully_correct_count\": fully_correct,\n",
    "        \"fully_correct_pct\": 100 * fully_correct / total,\n",
    "    }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0caaf9ca-15a5-46e7-9f30-fe676dab1f66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build correctness summary\n",
    "correctness_results = []\n",
    "\n",
    "for strategy, df in [\n",
    "    (\"P0\", cr_df_P0),\n",
    "    (\"P1\", cr_df_P1),\n",
    "    (\"P2\", cr_df_P2),\n",
    "    (\"P3\", cr_df_P3),\n",
    "]:\n",
    "    r = correctness_rates(df)\n",
    "    r[\"strategy\"] = strategy\n",
    "    correctness_results.append(r)\n",
    "\n",
    "correctness_summary = pd.DataFrame(correctness_results)\n",
    "\n",
    "# Paper-ready table (compact)\n",
    "paper_correctness = correctness_summary[[\n",
    "    \"strategy\",\n",
    "    \"total_tests\",\n",
    "    \"syntactic_not_ok_pct\",\n",
    "    \"execution_not_ok_pct\",\n",
    "    \"assertion_not_ok_executed_pct\",\n",
    "    \"fully_correct_pct\",\n",
    "]].copy()\n",
    "\n",
    "paper_correctness = paper_correctness.rename(columns={\n",
    "    \"strategy\": \"Strategy\",\n",
    "    \"total_tests\": \"N tests\",\n",
    "    \"syntactic_not_ok_pct\": \"Syntactic not ok (\\\\%)\",\n",
    "    \"execution_not_ok_pct\": \"Execution not ok* (\\\\%)\",\n",
    "    \"assertion_not_ok_executed_pct\": \"Assertion not ok** (\\\\%)\",\n",
    "    \"fully_correct_pct\": \"Fully correct (\\\\%)\",\n",
    "})\n",
    "\n",
    "paper_correctness = paper_correctness.round(2)\n",
    "\n",
    "paper_correctness\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab76d493-744c-4c93-8517-4ac8639e21f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Print table in template required latex form\n",
    "\n",
    "def df_to_latex_tabular(df: pd.DataFrame) -> str:\n",
    "    cols = df.columns.tolist()\n",
    "    col_format = \"|l|\" + \"|\".join([\"c\"] * (len(cols) - 1)) + \"|\"\n",
    "\n",
    "    lines = []\n",
    "    lines.append(f\"\\\\begin{{tabular}}{{{col_format}}}\")\n",
    "    lines.append(\"\\\\hline\")\n",
    "\n",
    "    # Header\n",
    "    header = \" & \".join(cols) + \" \\\\\\\\\"\n",
    "    lines.append(header)\n",
    "    lines.append(\"\\\\hline\")\n",
    "\n",
    "    # Rows\n",
    "    for _, row in df.iterrows():\n",
    "        row_str = \" & \".join(str(v) for v in row.values) + \" \\\\\\\\\"\n",
    "        lines.append(row_str)\n",
    "        lines.append(\"\\\\hline\")\n",
    "\n",
    "    lines.append(\"\\\\end{tabular}\")\n",
    "    return \"\\n\".join(lines)\n",
    "\n",
    "\n",
    "latex_tabular = df_to_latex_tabular(paper_correctness)\n",
    "print(latex_tabular)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0303fcb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def save_latex(path: str, content: str):\n",
    "    os.makedirs(os.path.dirname(path), exist_ok=True)\n",
    "    with open(path, \"w\") as f:\n",
    "        f.write(content)\n",
    "\n",
    "#Save table txt in the repository\n",
    "save_latex(\n",
    "    \"latex/tables/API/correctness.tex\",\n",
    "    latex_tabular\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad759285",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paper-ready correctness table (absolute counts + one percentage column)\n",
    "\n",
    "paper_correctness_counts = correctness_summary[[\n",
    "    \"strategy\",\n",
    "    \"total_tests\",\n",
    "    \"syntactic_not_ok_count\",\n",
    "    \"execution_not_ok_count\",\n",
    "    \"assertion_not_ok_executed_count\",\n",
    "    \"fully_correct_count\",\n",
    "]].copy()\n",
    "\n",
    "# Add: percentage of tests that are NOT fully correct\n",
    "paper_correctness_counts[\"not_fully_correct_pct\"] = (\n",
    "    100.0\n",
    "    * (paper_correctness_counts[\"total_tests\"]\n",
    "       - paper_correctness_counts[\"fully_correct_count\"])\n",
    "    / paper_correctness_counts[\"total_tests\"]\n",
    ")\n",
    "\n",
    "paper_correctness_counts = paper_correctness_counts.rename(columns={\n",
    "    \"strategy\": \"Strategy\",\n",
    "    \"total_tests\": \"N tests\",\n",
    "    \"syntactic_not_ok_count\": \"Syntactic not ok\",\n",
    "    \"execution_not_ok_count\": \"Execution not ok*\",\n",
    "    \"assertion_not_ok_executed_count\": \"Assertion not ok**\",\n",
    "    \"fully_correct_count\": \"Fully correct\",\n",
    "    \"not_fully_correct_pct\": \"Not fully correct (\\\\%)\",\n",
    "})\n",
    "\n",
    "paper_correctness_counts = paper_correctness_counts.round({\n",
    "    \"Not fully correct (\\\\%)\": 2\n",
    "})\n",
    "\n",
    "paper_correctness_counts\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b63d1d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "\n",
    "\n",
    "def save_latex(path: str, content: str):\n",
    "    os.makedirs(os.path.dirname(path), exist_ok=True)\n",
    "    with open(path, \"w\") as f:\n",
    "        f.write(content)\n",
    "\n",
    "latex_tabular_absolute = df_to_latex_tabular(paper_correctness_counts)\n",
    "print(latex_tabular_absolute)\n",
    "\n",
    "#Save table txt in the repository\n",
    "save_latex(\n",
    "    \"latex/tables/API/correctness_absolute2.tex\",\n",
    "    latex_tabular_absolute\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79386325-1fab-4d30-880c-7d5786676803",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Graph for correctness percentages\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plot_df = correctness_summary.set_index(\"strategy\")[\n",
    "    [\"syntactic_not_ok_pct\", \"execution_not_ok_pct\", \"assertion_not_ok_executed_pct\"]\n",
    "]\n",
    "\n",
    "colors = ['deepskyblue', 'royalblue', 'midnightblue']\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(7, 4.5))\n",
    "\n",
    "plot_df.plot(\n",
    "    kind=\"bar\",\n",
    "    ax=ax,\n",
    "    color=colors,\n",
    "    width=0.7\n",
    ")\n",
    "\n",
    "ax.set_ylabel(\"Not ok rate (%)\", fontsize=11, labelpad=10)\n",
    "ax.set_xlabel(\"Prompt strategy\", fontsize=11, labelpad=10)\n",
    "\n",
    "ax.set_title(\n",
    "    \"Correctness not ok rates by prompt strategy\",\n",
    "    pad=25\n",
    ")\n",
    "\n",
    "ax.set_ylim(0, plot_df.max().max() * 1.25)\n",
    "\n",
    "ax.legend(\n",
    "    [\n",
    "        \"Syntactic not ok (all tests)\",\n",
    "        \"Execution not ok (syntactic-valid tests)\",\n",
    "        \"Assertion not ok (executed tests only)\"\n",
    "    ],\n",
    "    loc=\"upper center\",\n",
    "    bbox_to_anchor=(0.5, 1.05),\n",
    "    frameon=False\n",
    ")\n",
    "\n",
    "ax.spines[\"top\"].set_visible(False)\n",
    "ax.spines[\"right\"].set_visible(False)\n",
    "\n",
    "fig.subplots_adjust(top=0.82)\n",
    "\n",
    "#Save image in the repository\n",
    "plt.savefig(\n",
    "    \"latex/figures/API/correctness_bar.png\",\n",
    "    dpi=300,\n",
    "    bbox_inches=\"tight\"\n",
    ")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08d227be-7282-4ebc-9c96-b6b8a2b8b5cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Show which kind of execution errors did the LLM struggle with the most (assertion errors not included)\n",
    "\n",
    "def categorize_error(msg) -> str:\n",
    "    # Handles NaN/None and non-strings safely\n",
    "    msg = \"\" if pd.isna(msg) else str(msg)\n",
    "    msg_l = msg.lower()\n",
    "\n",
    "    if \"valueerror\" in msg_l:\n",
    "        return \"ValueError\"\n",
    "    if \"attributeerror\" in msg_l:\n",
    "        return \"AttributeError\"\n",
    "    if \"typeerror\" in msg_l:\n",
    "        return \"TypeError\"\n",
    "    if \"assertionerror\" in msg_l or \"assert\" in msg_l:\n",
    "        return \"AssertionFailure\"\n",
    "\n",
    "    return \"Other\"\n",
    "\n",
    "\n",
    "error_summary = []\n",
    "\n",
    "for strategy, df in [(\"P0\", cr_df_P0), (\"P1\", cr_df_P1), (\"P2\", cr_df_P2), (\"P3\", cr_df_P3)]:\n",
    "    exec_fail_df = df[(df[\"syntactic_ok\"] == 1) & (df[\"execution_ok\"] == 0)]\n",
    "\n",
    "    categories = (\n",
    "        exec_fail_df[\"error_message\"]\n",
    "        .fillna(\"\")\n",
    "        .apply(categorize_error)\n",
    "    )\n",
    "\n",
    "    counts = categories.value_counts()\n",
    "\n",
    "    for cat, count in counts.items():\n",
    "        error_summary.append({\n",
    "            \"strategy\": strategy,\n",
    "            \"error_category\": cat,\n",
    "            \"count\": count\n",
    "        })\n",
    "\n",
    "error_summary_df = pd.DataFrame(error_summary)\n",
    "error_summary_df\n",
    "\n",
    "if error_summary_df.empty:\n",
    "    print(\"No execution not ok tests found (after filtering syntactic_ok==1 & execution_ok==0).\")\n",
    "else:\n",
    "    pivot = error_summary_df.pivot(\n",
    "        index=\"error_category\",\n",
    "        columns=\"strategy\",\n",
    "        values=\"count\"\n",
    "    ).fillna(0)\n",
    "\n",
    "    pivot.astype(int)\n",
    "\n",
    "    pivot.plot(kind=\"bar\")\n",
    "\n",
    "    plt.ylabel(\"Number of execution not ok tests\")\n",
    "    plt.xlabel(\"Error category\")\n",
    "    plt.title(\"Execution error categories by prompt strategy\")\n",
    "    plt.xticks(rotation=0)\n",
    "\n",
    "    # Save image in the repository (will fail if directory doesn't exist)\n",
    "    plt.savefig(\n",
    "        \"latex/figures/API/correctness_error_types.png\",\n",
    "        dpi=300,\n",
    "        bbox_inches=\"tight\"\n",
    "    )\n",
    "\n",
    "    plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
