You are given a target function from a Python library about HTTP requests.
Additionally, you are also given several examples of test cases for the target function. 

Your task is to use pytest and generate complete, executable and unique test cases for this target function. 
Use the given example test cases as guidance for structure, style, completeness, and formatting.

The Python version is 3.11
The pytest version is 8.4.2

=================================================================================================================
Target function name:
{{FUNCTION_NAME}}



Target function definition:
{{FUNCTION_DEFINITION}}



Target function module name:
{{FUNCTION_MODULE}}


Imports used by the module that defines {{FUNCTION_NAME}}:
{{IMPORTS}}



Dependency or helper functions for the target function (for context only):
{{DEPENDENCIES}}


Setup notes for successful execution:
{{SETUP_NOTES}}


Example test cases for the target function:
{{EXAMPLE_TESTS}}

=================================================================================================================

First, think through carefully, STEP BY STEP. 

BEFORE writing any code, perform a static analysis of the function {{FUNCTION_NAME}} provided above (INTERNALLY ONLY; do NOT output this analysis):
- List every conditional branch (if, elif, else) in the code.
- List every exception that can be raised (raise, try/except).
- List every loop condition.
- For each item in your list, describe a specific input that would trigger that path.
- Then, generate a test suite that covers these scenarios (subject to the one-test-per-file rule below).


For generating the tests, follow these rules:

HARD CONSTRAINTS (non-negotiable):
1. You MUST output one or more test files. Each test file is delimited by:
===TESTCASE_FILE_START===
<python code>
===TESTCASE_FILE_END===

2. Inside EACH test file:
- There MUST be EXACTLY ONE function definition whose name starts with "test_".
- Exactly one "def test_" is allowed.
- ZERO other "def test_" functions are allowed.
- No nested test functions.
- The file MUST test EXACTLY ONE scenario / behavior.
- Do NOT combine multiple cases in one test.
- No "Case 1/Case 2/..." sections.
- No multiple input variations in one test.
- No multiple independent asserts for different behaviors. (Multiple asserts are allowed ONLY if they all validate the SAME single scenario, e.g., value equality + type.)

3. Self-containment rules: 
- The test file MUST be executable as-is by pytest.
- Do not rely on shared state between test cases.
- You MUST NOT paste or reimplement {{FUNCTION_NAME}} in the test file.
- Include all necessary imports correctly to avoid collection failures.
- Dependency/helper functions are provided for context only; do NOT write tests for them.
- For all generated tests, {{FUNCTION_NAME}} is the unit under test.
- Do NOT reimplement dependency functions. Mock only external side effects (e.g., network I/O, randomness, time); avoid mocking core library objects and protocol artifacts. Prefer creating real library objects via public constructors and preparation methods.
- Construct any required "prepared" artifacts using the library’s public APIs, not by setting internal fields directly. Examples of correct patterns (generic, not function-specific):
	- Build requests via a Request → prepare() flow, or call a Prepared object’s prepare(...) before accessing/mutating headers/body.
	- Attach objects to Response fields only after creating valid instances (e.g., set response.request to a prepared request).
- If you use mocks, ensure they expose every attribute actually accessed AND that accessed attributes have correct primitive types (str/int/bytes/dict) rather than returning new Mock objects.
- Avoid chained mocks: do not rely on mock.attr.method().other_attr() chains for values used by the function. Use real primitives.
- Respect type contracts: when type checks exist, prefer real instances or official factories rather than mocks.
- Preserve container semantics: Do not replace library-provided containers (e.g., specialized maps, cookie jars, lists) with plain built-ins. Mutate the existing container in place so its case-insensitivity, normalization, and hooks remain intact.
- Prefer robust assertions: if outputs may be normalized/escaped (e.g., URLs, headers, whitespace/escapes), assert semantic invariants using appropriate parsing utilities instead of brittle full-string equality, unless the exact output is specified by the function contract.
- Comply with the setup notes. 

4. Output rules:
- Do not include Markdown code fences (```), output raw Python files only.
- Do NOT output any text outside the delimiters.
- Every delimiter block must be a complete standalone python file.


5. Quality target:
- Focus on a single case per generated test file. 
- The human-written test suite for the selected subset of 10 target functions contains 72 tests in total (≈7 tests per function on average, though the distribution is uneven).
- When executed in isolation, these human-written tests achieve approximately 62% coverage measured against the entire library.
- Human-written tests cover on average approximately 19 executable lines per test.
- These values are provided as descriptive context only and are not strict per-function or per-test requirements.
- Use good security hygiene. 

Now first think step by step (including the internal static analysis) and then generate multiple UNIQUE test files (each file = exactly one test function, exactly one scenario) for {{FUNCTION_NAME}}.
Do NOT print the static analysis. Output must contain only test delimiter blocks.
Validate the hard constraints internally before you output.
Follow the provided test cases as example structure.
Do not limit test scenarios to only those covered by the example tests.



